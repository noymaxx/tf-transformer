# Transformer PT‚ÜíEN com TensorFlow ‚Äî Notebook em 3 Steps

Implementa√ß√£o compacta de um **Transformer** para tradu√ß√£o **Portugu√™s ‚Üí Ingl√™s**, inspirada no tutorial oficial do TensorFlow Text. 

---

## üîß Requisitos

- Python 3.10+
- TensorFlow 2.16+ (ou vers√£o est√°vel do Colab)
- (Opcional) GPU com drivers NVIDIA + CUDA/cuDNN compat√≠veis
- Pacotes:
  ```
  tensorflow==2.16.1
  tensorflow-text==2.16.1
  tensorflow-datasets
  numpy
  matplotlib
  sacrebleu
  jupyter
  ```

---

## üóÇ Estrutura Sugerida do Reposit√≥rio

```
.
‚îú‚îÄ notebooks/
‚îÇ  ‚îî‚îÄ transformer.ipynb
‚îú‚îÄ scripts/
‚îÇ  ‚îú‚îÄ run_cpu.sh
‚îÇ  ‚îú‚îÄ run_gpu.sh
‚îÇ  ‚îú‚îÄ run_notebook_cpu.sh
‚îÇ  ‚îú‚îÄ run_notebook_gpu.sh
‚îÇ  ‚îî‚îÄ export_savedmodel.sh
‚îú‚îÄ requirements.txt
‚îú‚îÄ README.md
‚îî‚îÄ .gitignore
```

> Observa√ß√£o: toda a solu√ß√£o funciona apenas com o **notebook**. Os `scripts/` s√£o utilit√°rios para rodar localmente e comparar CPU vs GPU.

---

## ‚ñ∂Ô∏è Como Executar
1. **Setup & Data**
   - Checa GPU, baixa `ted_hrlr_translate/pt_to_en` via TFDS, aplica `TextVectorization` com `[START]`/`[END]`, cria `tf.data`.
2. **Model & Train**
   - Implementa Transformer Encoder‚ÄìDecoder (Keras), compila (Adam + CE), treina por poucas √©pocas e **mede tempo total**.
3. **Evaluate & Timing**
   - Decodifica√ß√£o *greedy* (PT‚ÜíEN), exemplos qualitativos, BLEU (opcional, subset) e **resumo de timing** (CPU vs GPU).

---

## üíª Execu√ß√£o R√°pida (Local)

```bash
# 1) Ambiente
python -m venv .venv
source .venv/bin/activate       # Windows: .venv\Scripts\activate

# 2) Depend√™ncias
pip install -r requirements.txt

# 3) Executar notebook manualmente (Jupyter/VSCode) e rodar STEP 1‚Üí2‚Üí3
```

**Autom√°tico (executa notebook pelo shell e salva c√≥pia executada):**
```bash
# CPU
EPOCHS=3 bash scripts/run_notebook_cpu.sh notebooks/transformer_translation_3steps.ipynb

# GPU
EPOCHS=3 bash scripts/run_notebook_gpu.sh notebooks/transformer_translation_3steps.ipynb
```

> Alternativa (se voc√™ tiver um `src/train.py`):  
> `bash scripts/run_cpu.sh --epochs 3 --batch-size 64`  
> `bash scripts/run_gpu.sh --epochs 3 --batch-size 64`

---

## Avalia√ß√£o & Exemplos

- **Infer√™ncia**: decodifica√ß√£o *greedy* no imprime pares PT / PRED EN / REF EN.
- **BLEU (opcional)**: usa `sacrebleu` em subset (100‚Äì1000 senten√ßas) para ter uma no√ß√£o de qualidade.

---

## Pontos Positivos 

- Integra√ß√£o **TFDS** simplifica download/preparo do dataset.
- `TextVectorization` resolve tokeniza√ß√£o b√°sica de forma r√°pida e reproduz√≠vel.
- Modelo Transformer em Keras √© leg√≠vel e modular; f√°cil de ajustar camadas/heads.
- **GPU** traz ganho significativo de tempo em batches m√©dios/grandes.
- Organiza√ß√£o em **3 passos** deixa o fluxo pedag√≥gico e f√°cil de depurar.

## Pontos Negativos 

- Compatibilidade entre **tensorflow** e **tensorflow-text** √© sens√≠vel a vers√µes (cuidado no `requirements.txt`).
- Setup local de **CUDA/cuDNN** pode ser demorado; Colab reduz atrito, mas a performance varia conforme o runtime.
- M√°scaras/aten√ß√£o podem gerar mensagens de erro pouco claras para iniciantes.
- BLEU confi√°vel demanda padroniza√ß√£o de tokeniza√ß√£o e mais dados/√©pocas.
- Treinos curtos servem mais para **compara√ß√£o de tempo** do que para avaliar qualidade final.


