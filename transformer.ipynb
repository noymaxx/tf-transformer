{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer \n",
        "\n",
        "Este notebook implementa um modelo **Transformer** para tradução (Pt→En).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29ec20d4",
      "metadata": {},
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a61b1692",
      "metadata": {},
      "source": [
        "**Objetivos do Step 1** - Preparar ambiente e checar GPU - Baixar e preparar o dataset `ted_hrlr_translate/pt_to_en` (TFDS) - Tokenizar com `TextVectorization` (simples e robusto) - Criar `tf.data` pipelines|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, time, math, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "try:\n",
        "    import tensorflow_text as tf_text  \n",
        "    HAS_TF_TEXT = True\n",
        "except Exception as e:\n",
        "    HAS_TF_TEXT = False\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"GPU disponível:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"tensorflow_text disponível:\", HAS_TF_TEXT)\n",
        "\n",
        "# Reprodutibilidade básica\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hiperparâmetros de dados\n",
        "MAX_VOCAB = 20000\n",
        "MAX_TOKENS = 100  # tamanho máximo da sequência (tokens)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# Carregue o dataset TED HRLR (Português → Inglês)\n",
        "# Splits: 'train', 'validation', 'test'\n",
        "ds_train, ds_val, ds_test = tfds.load(\n",
        "    \"ted_hrlr_translate/pt_to_en\",\n",
        "    split=[\"train\", \"validation\", \"test\"],\n",
        "    as_supervised=True  # retorna tuplas (pt, en)\n",
        ")\n",
        "\n",
        "def to_numpy_text(ds, take=-1):\n",
        "    xs_pt, ys_en = [], []\n",
        "    for i, (pt, en) in enumerate(ds):\n",
        "        xs_pt.append(pt.numpy().decode(\"utf-8\"))\n",
        "        ys_en.append(en.numpy().decode(\"utf-8\"))\n",
        "        if take > 0 and i+1 >= take:\n",
        "            break\n",
        "    return xs_pt, ys_en\n",
        "\n",
        "LIMIT = None  \n",
        "if LIMIT is not None:\n",
        "    xpt_train, yen_train = to_numpy_text(ds_train, take=LIMIT)\n",
        "    xpt_val,  yen_val  = to_numpy_text(ds_val,  take=max(1000, LIMIT//10))\n",
        "else:\n",
        "    xpt_train, yen_train = to_numpy_text(ds_train)\n",
        "    xpt_val,  yen_val  = to_numpy_text(ds_val)\n",
        "\n",
        "print(f\"Amostras de treino: {len(xpt_train)}, validação: {len(xpt_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Tokens especiais\n",
        "START_TOKEN = \"[START]\"\n",
        "END_TOKEN = \"[END]\"\n",
        "\n",
        "def add_special_tokens(texts, start=START_TOKEN, end=END_TOKEN):\n",
        "    return [f\"{start} \" + t.strip() + f\" {end}\" for t in texts]\n",
        "\n",
        "# Vetorizadores separados para PT (source) e EN (target)\n",
        "src_vectorizer = TextVectorization(\n",
        "    max_tokens=MAX_VOCAB,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_TOKENS,\n",
        "    standardize=\"lower_and_strip_punctuation\"\n",
        ")\n",
        "tgt_vectorizer = TextVectorization(\n",
        "    max_tokens=MAX_VOCAB,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_TOKENS,\n",
        "    standardize=\"lower_and_strip_punctuation\"\n",
        ")\n",
        "\n",
        "# Fit nos textos\n",
        "src_vectorizer.adapt(xpt_train)\n",
        "tgt_vectorizer.adapt(add_special_tokens(yen_train))  # target recebe [START]/[END]\n",
        "\n",
        "vocab_src = src_vectorizer.get_vocabulary()\n",
        "vocab_tgt = tgt_vectorizer.get_vocabulary()\n",
        "PAD_ID = 0  # por padrão TextVectorization usa 0 para padding\n",
        "\n",
        "print(\"Vocabulário (src) tamanho:\", len(vocab_src))\n",
        "print(\"Vocabulário (tgt) tamanho:\", len(vocab_tgt))\n",
        "print(\"Exemplo vocab tgt (0..10):\", vocab_tgt[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construa tf.data com pares (enc_inputs, dec_inputs) e labels (dec_targets)\n",
        "def make_dataset(x_src_raw, y_tgt_raw, batch_size=BATCH_SIZE):\n",
        "    y_tgt_in = add_special_tokens(y_tgt_raw)  # [START] ... [END]\n",
        "    # Vetorize\n",
        "    x_src = src_vectorizer(np.array(x_src_raw))\n",
        "    y_all = tgt_vectorizer(np.array(y_tgt_in))  # contém START + ... + END\n",
        "\n",
        "    # Dec inputs = tudo exceto o último token\n",
        "    dec_inputs = y_all[:, :-1]\n",
        "    # Labels/targets = tudo exceto o primeiro token\n",
        "    dec_targets = y_all[:, 1:]\n",
        "\n",
        "    # Máscara de loss (1 onde target != PAD, 0 caso contrário)\n",
        "    sample_weights = tf.cast(tf.not_equal(dec_targets, PAD_ID), tf.float32)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((x_src, dec_inputs), dec_targets, sample_weights))\n",
        "    ds = ds.shuffle(min(len(x_src_raw), BUFFER_SIZE)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_dataset(xpt_train, yen_train)\n",
        "val_ds   = make_dataset(xpt_val,  yen_val)\n",
        "\n",
        "for (a, b, w) in train_ds.take(1):\n",
        "    print(\"Shapes enc_in, dec_in:\", a[0].shape, a[1].shape)\n",
        "    print(\"Shapes y, w:\", b.shape, w.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c092aa64",
      "metadata": {},
      "source": [
        "## Model & Train\n",
        "\n",
        "- Construir um Transformer Encoder-Decoder (Keras)\n",
        "- Compilar com otimizador Adam e loss de entropia cruzada \n",
        "- Treinar por poucas épocas e registrar tempos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model, optimizers\n",
        "\n",
        "# Positional Encoding (sinusoidal)\n",
        "def positional_encoding(maxlen, d_model):\n",
        "    pos = np.arange(maxlen)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "    # aplica sin aos índices pares e cos aos índices ímpares\n",
        "    sines = np.sin(angle_rads[:, 0::2])\n",
        "    coses = np.cos(angle_rads[:, 1::2])\n",
        "    pe = np.zeros((maxlen, d_model))\n",
        "    pe[:, 0::2] = sines\n",
        "    pe[:, 1::2] = coses\n",
        "    return tf.cast(pe[np.newaxis, ...], tf.float32)  # shape: (1, maxlen, d_model)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model, maxlen):\n",
        "        super().__init__()\n",
        "        self.tok_emb = layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "        self.pos_enc = positional_encoding(maxlen, d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x = self.tok_emb(x)  # (B, L, d_model)\n",
        "        return x + self.pos_enc[:, :length, :]\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(dff, activation='relu'),\n",
        "            layers.Dense(d_model),\n",
        "        ])\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.drop1 = layers.Dropout(dropout)\n",
        "        self.drop2 = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training, padding_mask=None):\n",
        "        attn_output = self.att(x, x, attention_mask=padding_mask, training=training)\n",
        "        x = self.norm1(x + self.drop1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.drop2(ffn_output, training=training))\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout)\n",
        "        self.cross_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(dff, activation='relu'),\n",
        "            layers.Dense(d_model),\n",
        "        ])\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.drop1 = layers.Dropout(dropout)\n",
        "        self.drop2 = layers.Dropout(dropout)\n",
        "        self.drop3 = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, enc_out, training, padding_mask=None, look_ahead_mask=None):\n",
        "        # Máscara causal (look-ahead)\n",
        "        attn1 = self.self_att(x, x, attention_mask=look_ahead_mask, use_causal_mask=True, training=training)\n",
        "        x = self.norm1(x + self.drop1(attn1, training=training))\n",
        "\n",
        "        attn2 = self.cross_att(x, enc_out, attention_mask=padding_mask, training=training)\n",
        "        x = self.norm2(x + self.drop2(attn2, training=training))\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm3(x + self.drop3(ffn_out, training=training))\n",
        "        return x\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    # seq: (B, L)\n",
        "    mask = tf.cast(tf.not_equal(seq, PAD_ID), tf.float32)\n",
        "    # MultiHeadAttention espera máscara como (B, L) -> broadcasting para (B, 1, 1, L) é automático\n",
        "    return mask\n",
        "\n",
        "# Hiperparâmetros do modelo (pequenos para rodar rápido)\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "DFF = 512\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.1\n",
        "\n",
        "VOCAB_SRC = len(vocab_src)\n",
        "VOCAB_TGT = len(vocab_tgt)\n",
        "\n",
        "# Entradas\n",
        "enc_inputs = layers.Input(shape=(None,), dtype=\"int32\", name=\"enc_inputs\")\n",
        "dec_inputs = layers.Input(shape=(None,), dtype=\"int32\", name=\"dec_inputs\")\n",
        "\n",
        "# Embeddings + Positional Encoding\n",
        "enc_emb = TokenAndPositionEmbedding(VOCAB_SRC, D_MODEL, MAX_TOKENS)(enc_inputs)\n",
        "dec_emb = TokenAndPositionEmbedding(VOCAB_TGT, D_MODEL, MAX_TOKENS)(dec_inputs)\n",
        "\n",
        "# Encoder stack\n",
        "enc_out = enc_emb\n",
        "enc_mask = create_padding_mask(enc_inputs)\n",
        "for _ in range(NUM_LAYERS):\n",
        "    enc_out = TransformerEncoder(D_MODEL, NUM_HEADS, DFF, DROPOUT)(enc_out, training=True, padding_mask=enc_mask)\n",
        "\n",
        "# Decoder stack\n",
        "dec_out = dec_emb\n",
        "for _ in range(NUM_LAYERS):\n",
        "    dec_out = TransformerDecoder(D_MODEL, NUM_HEADS, DFF, DROPOUT)(dec_out, enc_out, training=True, padding_mask=enc_mask)\n",
        "\n",
        "# Saída final\n",
        "logits = layers.Dense(VOCAB_TGT, name=\"classifier\")(dec_out)  # (B, L, vocab_tgt)\n",
        "\n",
        "model = Model([enc_inputs, dec_inputs], logits, name=\"tiny_transformer_pt_en\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compilar\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
        "optimizer = optimizers.Adam(learning_rate=2e-4)\n",
        "\n",
        "# Função de perda que ignora padding via sample_weight (já fornecido no dataset)\n",
        "def masked_loss(y_true, y_pred):\n",
        "    # y_true: (B, L), y_pred: (B, L, V)\n",
        "    # Usaremos 'sample_weight' no .fit, então aqui apenas calculamos a loss padrão por timestep\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "    # shape (B, L). A média ponderada ocorrerá via sample_weight\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=masked_loss, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# Treinar poucas épocas para comparação de tempo (ajuste conforme necessário)\n",
        "EPOCHS = int(os.environ.get(\"EPOCHS\", 3))\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1\n",
        ")\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "train_seconds = t1 - t0\n",
        "print(f\"Tempo total de treino: {train_seconds:.2f} s em {EPOCHS} épocas.\")\n",
        "# Salve para uso no Step 3\n",
        "TRAIN_SECONDS = train_seconds\n",
        "EPOCHS_RAN = EPOCHS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate & CPU vs GPU Timing\n",
        "\n",
        "- Implementar inferência (decodificação greedy) para exemplos Pt→En\n",
        "- (Opcional) Calcular BLEU com `sacrebleu`\n",
        "- Registrar números de tempo de treino para comparação CPU vs GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapeios para detokenização\n",
        "inv_vocab_tgt = np.array(vocab_tgt)\n",
        "\n",
        "def detokenize_tgt(ids):\n",
        "    # Converte IDs de volta para tokens, remove [start]/[end] e padding\n",
        "    tokens = inv_vocab_tgt[ids]\n",
        "    tokens = [t for t in tokens if t not in (\"[start]\", \"[end]\", START_TOKEN.lower(), END_TOKEN.lower()) and t != \"\"]\n",
        "    # Reconstroi frase simples\n",
        "    return \" \".join(tokens).replace(\"  \", \" \").strip()\n",
        "\n",
        "def greedy_decode(pt_sentence, max_len=MAX_TOKENS):\n",
        "    # Tokeniza source\n",
        "    src = src_vectorizer(np.array([pt_sentence]))\n",
        "    dec = tgt_vectorizer(np.array([START_TOKEN]))\n",
        "    # 'dec' aqui vira sequência com [START] + padding; manteremos um buffer manual\n",
        "    dec_ids = [tgt_vectorizer.get_vocabulary().index(START_TOKEN.lower()) if START_TOKEN.lower() in tgt_vectorizer.get_vocabulary() else 1]\n",
        "\n",
        "    for _ in range(max_len-1):\n",
        "        dec_in = tf.constant([dec_ids + [0]*(max_len - len(dec_ids))], dtype=tf.int32)\n",
        "        logits = model([src, dec_in], training=False)  # (1, L, vocab)\n",
        "        next_id = int(tf.argmax(logits[0, len(dec_ids)-1]))  # pega o último passo\n",
        "        dec_ids.append(next_id)\n",
        "        # Se atingiu token END, para\n",
        "        tok = vocab_tgt[next_id] if next_id < len(vocab_tgt) else \"\"\n",
        "        if tok in (\"[end]\", END_TOKEN.lower()):\n",
        "            break\n",
        "    return detokenize_tgt(dec_ids)\n",
        "\n",
        "# Teste rápido com poucas amostras da validação\n",
        "samples = 5\n",
        "xpt_val_s, yen_val_s = xpt_val[:samples], yen_val[:samples]\n",
        "for i in range(samples):\n",
        "    src = xpt_val_s[i]\n",
        "    ref = yen_val_s[i]\n",
        "    pred = greedy_decode(src)\n",
        "    print(f\"PT: {src}\\nPRED EN: {pred}\\nREF  EN: {ref}\\n\" + \"-\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import sacrebleu\n",
        "    refs = [yen_val_s]\n",
        "    hyps = [greedy_decode(s) for s in xpt_val_s]\n",
        "    bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
        "    print(\"BLEU (subset pequeno):\", bleu.score)\n",
        "except Exception as e:\n",
        "    print(\"sacrebleu indisponível. Pule esta célula ou instale sacrebleu.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n",
        "print(\"Dispositivo detectado nesta execução:\", DEVICE)\n",
        "print(f\"Tempo total de treino nesta execução: {TRAIN_SECONDS:.2f} s para {EPOCHS_RAN} épocas.\")\n",
        "\n",
        "# Preencha manualmente para documentar no README do repositório:\n",
        "cpu_seconds = None   # ex.: 180.25\n",
        "gpu_seconds = None   # ex.: 42.10\n",
        "\n",
        "print(\"\\n👉 Dica: preencha cpu_seconds/gpu_seconds acima e rode novamente para imprimir o comparativo.\")\n",
        "\n",
        "if cpu_seconds is not None and gpu_seconds is not None:\n",
        "    speedup = cpu_seconds / gpu_seconds if gpu_seconds > 0 else float('inf')\n",
        "    print(f\"Comparativo: CPU={cpu_seconds:.2f}s, GPU={gpu_seconds:.2f}s, Speedup ≈ {speedup:.2f}x\")\n",
        "else:\n",
        "    print(\"Comparativo pendente — rode em ambos os modos (CPU e GPU) e registre os tempos.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
